import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, VotingRegressor, StackingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

# Load data from the local Excel file
df = pd.read_excel("Data_Set_2.xlsx")

# Extract input and output data
X = df[['Nodes', 'Radius', 'packet_sent', 'Packet_received', 'SF7', 'SF8', 'SF9', 'SF10', 'SF11', 'SF12']]
Y = df['Actual PDR']

# Impute missing values in the input data
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Handle missing values in the target variable Y
Y_imputed = np.nan_to_num(Y)

# Define models
models = {
    "Bagging": BaggingRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Voting": VotingRegressor([('lr', LinearRegression()), ('dt', DecisionTreeRegressor()), ('rf', RandomForestRegressor())]),
    "Stacking": StackingRegressor([('lr', LinearRegression()), ('dt', DecisionTreeRegressor()), ('rf', RandomForestRegressor())]),
    "AdaBoost": AdaBoostRegressor(),
    "Gradient Boosting": GradientBoostingRegressor(),
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "k-Nearest Neighbors": KNeighborsRegressor(),
}

# Add XGBoost and CatBoost to the models dictionary
models.update({
    "XGBoost": XGBRegressor(),
    "CatBoost": CatBoostRegressor(silent=True)  # Set silent=True to suppress output
})

# Aggregate predictions from all models and compute evaluation metrics
metrics = {}

# Perform train-test split for the entire dataset
X_train, X_test, Y_train, Y_test = train_test_split(X_imputed, Y_imputed, test_size=0.2, random_state=42)

# Scale input features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Plot predictions from each model in separate subplots without spline interpolation
node_values = np.unique(X_imputed[:, 0])  # Get unique 'Nodes' values from the dataset
for node_value in node_values:
    plt.figure(figsize=(10, 6))
    plt.title(f'Actual vs Predicted PDR for Nodes={node_value}')
    plt.xlabel('Radius')
    plt.ylabel('PDR')

    for model_name, model in models.items():
        # Fit the model for each output column individually
        Y_train_pred = np.zeros_like(Y_train)
        Y_test_pred = np.zeros_like(Y_test)
        
        # Filter train and test data for the current 'Nodes' value
        X_train_filtered = X_train[X_train[:, 0] == node_value]
        Y_train_filtered = Y_train[X_train[:, 0] == node_value]
        X_test_filtered = X_test[X_test[:, 0] == node_value]
        Y_test_filtered = Y_test[X_test[:, 0] == node_value]

        # Check if data points are available for the current 'Nodes' value
        if len(X_test_filtered) > 0:
            # Fit the model
            model.fit(X_train_scaled, Y_train)
            
            # Make predictions
            y_train_pred = model.predict(X_train_scaled)
            y_test_pred = model.predict(X_test_scaled)
            
            # Store predictions for the current 'Nodes' value
            Y_train_pred[X_train[:, 0] == node_value] = y_train_pred[X_train[:, 0] == node_value]
            Y_test_pred[X_test[:, 0] == node_value] = y_test_pred[X_test[:, 0] == node_value]
            
            # Calculate metrics
            mae_train = mean_absolute_error(Y_train_filtered, y_train_pred[X_train[:, 0] == node_value])
            mse_train = mean_squared_error(Y_train_filtered, y_train_pred[X_train[:, 0] == node_value])
            r2_train = r2_score(Y_train_filtered, y_train_pred[X_train[:, 0] == node_value])
            
            mae_test = mean_absolute_error(Y_test_filtered, y_test_pred[X_test[:, 0] == node_value])
            mse_test = mean_squared_error(Y_test_filtered, y_test_pred[X_test[:, 0] == node_value])
            r2_test = r2_score(Y_test_filtered, y_test_pred[X_test[:, 0] == node_value])
            
            metrics[model_name] = {
                "MAE Train": mae_train,
                "MSE Train": mse_train,
                "R2 Train": r2_train,
                "MAE Test": mae_test,
                "MSE Test": mse_test,
                "R2 Test": r2_test
            }
            
            # Plot predictions
            sorted_indices = np.argsort(X_test_filtered[:, 1])
            x_sorted = X_test_filtered[:, 1][sorted_indices]
            plt.plot(x_sorted, y_test_pred[X_test[:, 0] == node_value][sorted_indices], label=f'{model_name} Predicted')

    # Plot actual values as scatter points if available
    if len(X_test_filtered) > 0:
        plt.scatter(x_sorted, Y_test_filtered[sorted_indices], label='Actual', color='red', linestyle='--', linewidth=2)

    plt.legend()
    plt.show()

# Print evaluation metrics for each model
for model_name, model_metrics in metrics.items():
    print(f"Model: {model_name}")
    print(f"MAE Train: {model_metrics['MAE Train']}")
    print(f"MSE Train: {model_metrics['MSE Train']}")
    print(f"R2 Train: {model_metrics['R2 Train']}")
    print(f"MAE Test: {model_metrics['MAE Test']}")
    print(f"MSE Test: {model_metrics['MSE Test']}")
    print(f"R2 Test: {model_metrics['R2 Test']}")
    print()

